# LettiNote
Combination of Letta and MediNote

## What Certain Files Mean
This section was done by going through the project_copy.ipynb file and seeing which files are generated.

Note: that direct, generator, and summarizer folders contains data for fine-tuning.

Note: Not entirely sure about the extract() process in 1.4, but I believe that may be creating the gold patient summaries, since they used GPT-4 in their example. summaries_full_gen.jsonl is only found in the infered process, but I believe it is a part of augmented_notes_30k.jsonl. 

### data/direct
*direct_30K.jsonl* : Part of section '2 Preparing fine-tuning datasets'. This is for DIALOG --TO--> CLINCIAL NOTES. (Keys: idx, prompt, gold). *The "gold" refers to the ground truth clinical notes. Truncated due to from NoteChat.*
The ipynb file defines this jsonl as: 

"input is the direct prompt + synthetic patient-doctor dialog from NoteChat, output is the clinical note from PMC-Patients."

*direct_full_30K.jsonl* : Part of section 2.4 Full Clinical notes. Exactely the same as direct_30K.jsonl but with _full_ clinical ground truth notes, which is longer. (Keys: idx, prompt, gold). *The "gold" refers to the ground truth clinical notes.*

### data/generator
*generator_30K.jsonl* : Part of section '2 Preparing fine-tuning datasets'. This is for PATIENT SUMMARIES --TO--> CLINCIAL NOTES. (Keys: idx, prompt, gold). 
*The "gold" refers to the gold standard of the ground truth clinical notes.*
The ipynb file defines this jsonl as: 

"input is the generator prompt + patient summary from GPT-4, output is the clinical note from PMC-Patients. "

### data/inference

### data/summaries
*augmented_notes_30K.jsonl* : This dataset is well documented at
https://huggingface.co/datasets/AGBonnet/augmented-clinical-notes

Even though not explictly stated, I believe summaries_full_gen is meant to be augmented_notes_30K. Used at the beginning of section '2 Preparing fine-tuning datasets'. 

| Field | Description | Source |
|-|-|-|
| `idx` | Unique identifier, index in the original NoteChat-ChatGPT dataset | NoteChat |
| `note` | Clinical note used by NoteChat (possibly truncated) | NoteChat      |
| `full_note`       | Full clinical note | PMC-Patients |
| `conversation`  | Patient-doctor dialogue | NoteChat    |
| `summary`| Patient information summary (JSON) | ours |


*summaries_full_30k.jsonl* : This is generated in section 2.4 Full Clinical notes. In the generation, it is originally generated from summarizer_30K.jsonl, but I have changed it to be generated from augmented_notes_30K.jsonl since summarizer_30K.jsonl does not exist and is not created. When summaries_full_30k is being created, **full** clinical notes ground truth are added, which augmented_notes_30K has. 

### data/summarizer
*summarizer_30K.jsonl* : Part of section '2 Preparing fine-tuning datasets'. This is for DIALOG --TO--> PATIENT SUMMARIES, NOT CLINCIAL NOTES. (Keys: idx, prompt, gold). *The "gold" refers to the patient summaries generated by GPT-4.* The ipynb file defines this jsonl as: 

"input is the summarizer prompt + synthetic patient-doctor dialog from NoteChat, output is GPT-4's patient summary"

### data
*NoteChat.jsonl* : The NoteChat dataset, which are the clinical notes and docter patient conversations (Keys: note, converstation). 1.1

*NoteChat_sorted.jsonl* : The NoteChat dataset, which are the clinical notes and docter patient conversations BUT SORTED BY LENGTH OF CLINICAL NOTES. Goal, extract longest notes first since more information (Keys: note, converstation, length). 1.4

*PMC-Patients.jsonl* Generated in section 2.4, which is the full clinical note ground truth. (Keys; ('patient_id', 'patient_uid', 'PMID', 'file_path', 'title', 'patient', 'age', 'gender', 'relevant_articles', 'similar_patients')), 'patient' has the clinical notes. 

### generation/saves

*summaries_full_gen.jsonl* : The __pateint summaries__ generated by gpt-4-1106-preview. (Keys: idx, data, converstation, summary, note). I think the "data" key is deprecated. Additionally, the set of keys are similar to augmented_notes_30K.json, so I think that summaries_full_gen is meant to be augmented_notes_30K.

### Train and Test jsonl
These files are created from their respective original file names. 
See section 2.4 Train-Test Split

## Modifications to requirements.txt
Due to compatibility issues with my system, and with packages each other, I had to download the 'torch' and 'vllm' libraries separate with no specification.
I have updated the requirements.txt file to automatically do this, but if that does not work I would manually download them with pip install. 

Another library not listed in requirements.txt but is needed is multielo score library, which can be downloaded through: 
```bash
pip install git+https://github.com/djcunningham0/multielo.git
```

Need to download the following libraries as well. These are listed from https://github.com/google-research/google-research/blob/master/rouge/requirements.txt
```
absl-py
nltk
numpy
six>=1.14
```

lastly, I also downloaded rouge-score

## Modifications to the code (Still working on it)
Things that are changed have a #myadd next to the code changes.
There are 4 files that I have made changes so far. 

### project.ipynb
Changes to project.ipynb file can be found in project_copy.ipynb file.
In section 2 preparing database, ```summaries_30K.jsonl``` does not exist, so I pull the data from ```augmented_notes_30k.jsonl``` instead, which can be found on Hugging Face. I believe summaries_30K.jsonl was meant to be augmented_notes_30k.jsonl, since summaries_30K.jsonl is not found anywhere else and that it is mentioned after the extraction of patient summaries of clinical notes.

```python
#summaries_path = os.path.join(SUMMARIES_DIR, 'summaries_30K.jsonl')
summaries_path = os.path.join(SUMMARIES_DIR, 'augmented_notes_30k.jsonl') #myadd
```

Additionally MediNote code has the issue of using the keyword 'data' which does not exist in many of the datasets generated. Through process of elimination and educated guessing, I think 'data' ==> 'note'. 

```python
#summaries['data'] = summaries['data'].apply(lambda x: x.strip())
summaries['note'] = summaries['note'].apply(lambda x: x.strip()) #myadd
```

Section 2.2 and 2.3 have key errors, which I replace 'data' with 'note'

Section 2.2
```python
generator_path = os.path.join(GENERATOR_DIR, 'generator_30K.jsonl')
generator = prepare_dataset(data_path=summaries_path, 
                         save_path=generator_path,
                         prompt_path='generation/instructions/generate.txt',
                         prompt_key='summary',
                         #gold_key='data')
                         gold_key='note') #myadd
generator.head()
```
Section 2.3
```python
direct_path = os.path.join(DIRECT_DIR, 'direct_30K.jsonl')
direct = prepare_dataset(data_path=summaries_path, 
                      save_path=direct_path,
                      prompt_path='generation/instructions/direct.txt',
                      prompt_key='conversation',
                      #gold_key='data')
                      gold_key='note') #myadd
direct.head()
```

Section 2.4 have additional 'data' key error, and function argument error in length_histogram

```python
#length_histogram(full_summaries, key='full_note')
length_histogram(full_summaries_path, keys='full_note') #myadd
```

### data.py
line 197 has a key error.
```python
#summaries['data'] = summaries['data'].apply(lambda x: x.strip())
summaries['note'] = summaries['note'].apply(lambda x: x.strip()) #myadd
```

line 211 and 221 also have key error.
```python
    print(f'Preparing generator dataset...')
    generator_path = os.path.join(data_dir, 'generator', 'generator_30K.jsonl')
    generator = prepare_dataset(
        data_path=summaries_path, 
        save_path=generator_path,
        prompt_path='generation/instructions/generate.txt',
        prompt_key='summary',
        # gold_key='data')
        gold_key='note')#myadd

    print(f'Preparing direct dataset...')
    direct_path = os.path.join(data_dir, 'direct', 'direct_30K.jsonl')
    direct = prepare_dataset(
        data_path=summaries_path, 
        save_path=direct_path,
        prompt_path='generation/instructions/direct.txt',
        prompt_key='conversation',
        #gold_key='data')
        gold_key='note')#myadd
```

### generate.py

Missing function/library in this file. So I added the missing function.

```python
#myadd added extra load_file function from data, load file is undefined in this file, but is in data.py
def load_file(path): 
    '''
    Given a .csv or .json or .jsonl or .txt file,
    load it into a dataframe or string.
    '''
    if not os.path.exists(path):
        raise ValueError(f"Path {path} does not exist.")
    if '.csv' in path:
        data = pd.read_csv(path)
    elif '.jsonl' in path:
        data = pd.read_json(path, lines=True)
    elif '.json' in path:
        data = pd.read_json(path)
    elif '.txt' in path:
        with open(path, 'r') as f:
            data = f.read()
    else: 
        raise ValueError(f"Provided path {path} is not a valid file.")
    return data
```

### infer.py
Made changes to global variables since it raises key errors latter on.
```python
KEYS = {
    'summarizer': {
        'input': 'conversation',
        'output': 'pred_summary',
        'gold': 'summary'
    },
    'generator': {
        'input': 'pred_summary',
        'output': 'pred_note',
        'gold': 'data'
    },
    'generator-gpt': {
        # 'input': 'pred_summary',
        'input': 'summary', #myadd ,no pred_summary in summaries_full_gen.jsonl which is used in the load_data function 
        'output': 'pred_note_gpt',
        'gold': 'data'
    },
...
}
```
Made changes in the load_data function to account for data inconsistancies
```python
    if input_key not in data_df.columns:
        # raise ValueError(f'Input key {input_key} not found in output file.')
        raise ValueError(f'Input key {input_key} not found in input file.') #myadd
        # output file has not been loaded, so how it output file possible?
```

### scorer.py
Made changes to the GPT_ranker, the GPT_attribute_scorer and the GPT_similarity_scorer function, since the max tokens not correct for the model we are using.

```
    def GPT_attribute_scorer(
            self,
            pairs_df,
            score_type,
            sys_prompt,
            usr_prompt_func,
            model_name='gpt-4-1106-preview',
            chat=chat_gpt_4_turbo,
            temperature=0.0,
            # max_tokens=400000, 
            max_tokens=30000, #myadd
            one_call_batch_size=8,
            cot = True
            ):
```

```
    def GPT_similarity_scorer(self, 
                   pairs_df,
                   model_name='gpt-4-1106-preview',
                   chat=chat_gpt_4_turbo,
                   temperature=0.0,
                #    max_tokens = 400000, 
                   max_tokens = 30000, #myadd
                   one_call_batch_size=8,
                   cot = False):
```

```
    def GPT_ranker(self, 
                   pairs_df,
                   model_name='gpt-4-1106-preview',
                   chat=chat_gpt_4_turbo,
                   max_tokens = 400000,
                   max_tokens = 30000, #myadd
                   one_call_batch_size = 5, 
                   temperature=0.0,
                   cot = True):
```